{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-08T09:35:53.846259Z","iopub.execute_input":"2023-05-08T09:35:53.846717Z","iopub.status.idle":"2023-05-08T09:35:53.854512Z","shell.execute_reply.started":"2023-05-08T09:35:53.846675Z","shell.execute_reply":"2023-05-08T09:35:53.853258Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"__notebook_source__.ipynb\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd, numpy as np, re, pickle\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense\nimport json, re, string, unicodedata\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split as tts","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:35:53.856867Z","iopub.execute_input":"2023-05-08T09:35:53.857716Z","iopub.status.idle":"2023-05-08T09:36:02.375884Z","shell.execute_reply.started":"2023-05-08T09:35:53.857665Z","shell.execute_reply":"2023-05-08T09:36:02.374471Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"que, ans = [], []\nwith open('../input/simple-dialogs-for-chatbot/dialogs.txt','r') as f:\n    for line in f:\n        line = line.split('\\t')\n        que.append(line[0])\n        ans.append(line[1].replace('\\n',''))","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:02.377348Z","iopub.execute_input":"2023-05-08T09:36:02.378066Z","iopub.status.idle":"2023-05-08T09:36:02.408019Z","shell.execute_reply.started":"2023-05-08T09:36:02.378024Z","shell.execute_reply":"2023-05-08T09:36:02.406981Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({'que':que,'ans':ans})","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:02.410533Z","iopub.execute_input":"2023-05-08T09:36:02.410898Z","iopub.status.idle":"2023-05-08T09:36:02.419804Z","shell.execute_reply.started":"2023-05-08T09:36:02.410865Z","shell.execute_reply":"2023-05-08T09:36:02.418297Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def unicode_to_ascii(text):\n    return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:02.420997Z","iopub.execute_input":"2023-05-08T09:36:02.421322Z","iopub.status.idle":"2023-05-08T09:36:02.427343Z","shell.execute_reply.started":"2023-05-08T09:36:02.421291Z","shell.execute_reply":"2023-05-08T09:36:02.426308Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"contractions_dict = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"doesn’t\": \"does not\",\n\"don't\": \"do not\",\n\"don’t\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so is\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y’all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\",\n\"ain’t\": \"am not\",\n\"aren’t\": \"are not\",\n\"can’t\": \"cannot\",\n\"can’t’ve\": \"cannot have\",\n\"’cause\": \"because\",\n\"could’ve\": \"could have\",\n\"couldn’t\": \"could not\",\n\"couldn’t’ve\": \"could not have\",\n\"didn’t\": \"did not\",\n\"doesn’t\": \"does not\",\n\"don’t\": \"do not\",\n\"don’t\": \"do not\",\n\"hadn’t\": \"had not\",\n\"hadn’t’ve\": \"had not have\",\n\"hasn’t\": \"has not\",\n\"haven’t\": \"have not\",\n\"he’d\": \"he had\",\n\"he’d’ve\": \"he would have\",\n\"he’ll\": \"he will\",\n\"he’ll’ve\": \"he will have\",\n\"he’s\": \"he is\",\n\"how’d\": \"how did\",\n\"how’d’y\": \"how do you\",\n\"how’ll\": \"how will\",\n\"how’s\": \"how is\",\n\"i’d\": \"i would\",\n\"i’d’ve\": \"i would have\",\n\"i’ll\": \"i will\",\n\"i’ll’ve\": \"i will have\",\n\"i’m\": \"i am\",\n\"i’ve\": \"i have\",\n\"isn’t\": \"is not\",\n\"it’d\": \"it would\",\n\"it’d’ve\": \"it would have\",\n\"it’ll\": \"it will\",\n\"it’ll’ve\": \"it will have\",\n\"it’s\": \"it is\",\n\"let’s\": \"let us\",\n\"ma’am\": \"madam\",\n\"mayn’t\": \"may not\",\n\"might’ve\": \"might have\",\n\"mightn’t\": \"might not\",\n\"mightn’t’ve\": \"might not have\",\n\"must’ve\": \"must have\",\n\"mustn’t\": \"must not\",\n\"mustn’t’ve\": \"must not have\",\n\"needn’t\": \"need not\",\n\"needn’t’ve\": \"need not have\",\n\"o’clock\": \"of the clock\",\n\"oughtn’t\": \"ought not\",\n\"oughtn’t’ve\": \"ought not have\",\n\"shan’t\": \"shall not\",\n\"sha’n’t\": \"shall not\",\n\"shan’t’ve\": \"shall not have\",\n\"she’d\": \"she would\",\n\"she’d’ve\": \"she would have\",\n\"she’ll\": \"she will\",\n\"she’ll’ve\": \"she will have\",\n\"she’s\": \"she is\",\n\"should’ve\": \"should have\",\n\"shouldn’t\": \"should not\",\n\"shouldn’t’ve\": \"should not have\",\n\"so’ve\": \"so have\",\n\"so’s\": \"so is\",\n\"that’d\": \"that would\",\n\"that’d’ve\": \"that would have\",\n\"that’s\": \"that is\",\n\"there’d\": \"there would\",\n\"there’d’ve\": \"there would have\",\n\"there’s\": \"there is\",\n\"they’d\": \"they would\",\n\"they’d’ve\": \"they would have\",\n\"they’ll\": \"they will\",\n\"they’ll’ve\": \"they will have\",\n\"they’re\": \"they are\",\n\"they’ve\": \"they have\",\n\"to’ve\": \"to have\",\n\"wasn’t\": \"was not\",\n\"we’d\": \"we would\",\n\"we’d’ve\": \"we would have\",\n\"we’ll\": \"we will\",\n\"we’ll’ve\": \"we will have\",\n\"we’re\": \"we are\",\n\"we’ve\": \"we have\",\n\"weren’t\": \"were not\",\n\"what’ll\": \"what will\",\n\"what’ll’ve\": \"what will have\",\n\"what’re\": \"what are\",\n\"what’s\": \"what is\",\n\"what’ve\": \"what have\",\n\"when’s\": \"when is\",\n\"when’ve\": \"when have\",\n\"where’d\": \"where did\",\n\"where’s\": \"where is\",\n\"where’ve\": \"where have\",\n\"who’ll\": \"who will\",\n\"who’ll’ve\": \"who will have\",\n\"who’s\": \"who is\",\n\"who’ve\": \"who have\",\n\"why’s\": \"why is\",\n\"why’ve\": \"why have\",\n\"will’ve\": \"will have\",\n\"won’t\": \"will not\",\n\"won’t’ve\": \"will not have\",\n\"would’ve\": \"would have\",\n\"wouldn’t\": \"would not\",\n\"wouldn’t’ve\": \"would not have\",\n\"y’all\": \"you all\",\n\"y’all\": \"you all\",\n\"y’all’d\": \"you all would\",\n\"y’all’d’ve\": \"you all would have\",\n\"y’all’re\": \"you all are\",\n\"y’all’ve\": \"you all have\",\n\"you’d\": \"you would\",\n\"you’d’ve\": \"you would have\",\n\"you’ll\": \"you will\",\n\"you’ll’ve\": \"you will have\",\n\"you’re\": \"you are\",\n\"you’ve\": \"you have\",\n\"n't\": \"not\",\n\"n'\": \"ng\",\n\"'bout\": \"about\",\n\"'til\": \"until\"\n}","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:02.429276Z","iopub.execute_input":"2023-05-08T09:36:02.429690Z","iopub.status.idle":"2023-05-08T09:36:02.458149Z","shell.execute_reply.started":"2023-05-08T09:36:02.429654Z","shell.execute_reply":"2023-05-08T09:36:02.456597Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"con_re = re.compile('(%s)'%'|'.join(contractions_dict.keys()))\ndef expand_contractions(raw):\n    def replace(match):\n        return contractions_dict[match.group(0)]\n    return con_re.sub(replace, raw)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:02.459452Z","iopub.execute_input":"2023-05-08T09:36:02.459825Z","iopub.status.idle":"2023-05-08T09:36:02.478843Z","shell.execute_reply.started":"2023-05-08T09:36:02.459789Z","shell.execute_reply":"2023-05-08T09:36:02.477596Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = unicode_to_ascii(text)\n    text = expand_contractions(text)\n    text = ''.join(w for w in text if w not in string.punctuation).strip()\n    text =  \"<sos> \" +  text + \" <eos>\"\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:02.480255Z","iopub.execute_input":"2023-05-08T09:36:02.480606Z","iopub.status.idle":"2023-05-08T09:36:02.488424Z","shell.execute_reply.started":"2023-05-08T09:36:02.480565Z","shell.execute_reply":"2023-05-08T09:36:02.487484Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df.que = df.que.apply(clean_text)\ndf.ans = df.ans.apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:02.489742Z","iopub.execute_input":"2023-05-08T09:36:02.490073Z","iopub.status.idle":"2023-05-08T09:36:02.761813Z","shell.execute_reply.started":"2023-05-08T09:36:02.490041Z","shell.execute_reply":"2023-05-08T09:36:02.760727Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"que = df.que.values.tolist()\nans = df.ans.values.tolist()","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:02.765769Z","iopub.execute_input":"2023-05-08T09:36:02.766225Z","iopub.status.idle":"2023-05-08T09:36:02.772238Z","shell.execute_reply.started":"2023-05-08T09:36:02.766189Z","shell.execute_reply":"2023-05-08T09:36:02.771033Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def tokenize(lang):\n    token = tf.keras.preprocessing.text.Tokenizer(filters='')\n    token.fit_on_texts(lang)\n    tensor = token.texts_to_sequences(lang)\n    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n    return tensor, token","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:02.773899Z","iopub.execute_input":"2023-05-08T09:36:02.774480Z","iopub.status.idle":"2023-05-08T09:36:02.782122Z","shell.execute_reply.started":"2023-05-08T09:36:02.774434Z","shell.execute_reply":"2023-05-08T09:36:02.781117Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"input_tensor, input_lang = tokenize(que)\ntarget_tensor, target_lang = tokenize(ans)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:02.783409Z","iopub.execute_input":"2023-05-08T09:36:02.783747Z","iopub.status.idle":"2023-05-08T09:36:02.957517Z","shell.execute_reply.started":"2023-05-08T09:36:02.783714Z","shell.execute_reply":"2023-05-08T09:36:02.955952Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"max_len_target, max_len_inp = target_tensor.shape[1], input_tensor.shape[1]\nprint(max_len_target, max_len_inp)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:02.959201Z","iopub.execute_input":"2023-05-08T09:36:02.959529Z","iopub.status.idle":"2023-05-08T09:36:02.965137Z","shell.execute_reply.started":"2023-05-08T09:36:02.959497Z","shell.execute_reply":"2023-05-08T09:36:02.964104Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"22 22\n","output_type":"stream"}]},{"cell_type":"code","source":"xtr, xte, ytr, yte = tts(input_tensor, target_tensor, test_size=.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:02.966516Z","iopub.execute_input":"2023-05-08T09:36:02.966874Z","iopub.status.idle":"2023-05-08T09:36:02.977756Z","shell.execute_reply.started":"2023-05-08T09:36:02.966842Z","shell.execute_reply":"2023-05-08T09:36:02.976647Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"buffer_size = len(xtr)\nbatch_size = 64\nsteps_per_epoch = buffer_size//batch_size\nemb_dim = 256\nunits = 1024\n\nvoc_in_size = len(input_lang.word_index)+1\nvoc_tar_size = len(target_lang.word_index)+1\n\nds = tf.data.Dataset.from_tensor_slices((xtr,ytr)).shuffle(buffer_size)\nds = ds.batch(batch_size, drop_remainder=True)\n\nexm_in_bat, exm_tar_bat = next(iter(ds))\nexm_in_bat.shape, exm_tar_bat.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:02.979511Z","iopub.execute_input":"2023-05-08T09:36:02.980127Z","iopub.status.idle":"2023-05-08T09:36:03.179495Z","shell.execute_reply.started":"2023-05-08T09:36:02.980084Z","shell.execute_reply":"2023-05-08T09:36:03.178624Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(TensorShape([64, 22]), TensorShape([64, 22]))"},"metadata":{}}]},{"cell_type":"code","source":"@keras.utils.register_keras_serializable()\nclass Encoder(tf.keras.Model):\n    def __init__(self, voc_size, emb_dim, enc_units, batch_size):\n        super(Encoder, self).__init__()\n        self.batch_size = batch_size\n        self.enc_units = enc_units\n        self.embedding = tf.keras.layers.Embedding(voc_size, emb_dim)\n        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n        \n    def call(self, x, hidden):\n        x = self.embedding(x)\n        # out: A tensor of shape (batch_size, max_length, enc_units), which contains a vector of size enc_units for each word in the input sequence. \n        # These vectors represent the hidden states of the GRU layer at each time step.\n        out, state = self.gru(x, initial_state=hidden)\n        return out, state\n    \n    def initialize_hidden_state(self):\n        return tf.zeros((self.batch_size, self.enc_units))","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:03.181201Z","iopub.execute_input":"2023-05-08T09:36:03.181973Z","iopub.status.idle":"2023-05-08T09:36:03.191445Z","shell.execute_reply.started":"2023-05-08T09:36:03.181926Z","shell.execute_reply":"2023-05-08T09:36:03.190122Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"encoder = Encoder(voc_in_size, emb_dim, units, batch_size)\nsam_hidden = encoder.initialize_hidden_state()\nsam_output, sam_hidden = encoder(exm_in_bat, sam_hidden)\nprint ('Encoder output shape: (batch size, sequence length, units) {}'.format(sam_output.shape))\nprint ('Encoder Hidden state shape: (batch size, units) {}'.format(sam_hidden.shape))","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:03.193071Z","iopub.execute_input":"2023-05-08T09:36:03.193610Z","iopub.status.idle":"2023-05-08T09:36:03.855410Z","shell.execute_reply.started":"2023-05-08T09:36:03.193545Z","shell.execute_reply":"2023-05-08T09:36:03.853721Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Encoder output shape: (batch size, sequence length, units) (64, 22, 1024)\nEncoder Hidden state shape: (batch size, units) (64, 1024)\n","output_type":"stream"}]},{"cell_type":"code","source":"class BahdanauAttention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(BahdanauAttention,self).__init__()\n        self.W1=tf.keras.layers.Dense(units)\n        self.W2=tf.keras.layers.Dense(units)\n        self.V=tf.keras.layers.Dense(1)\n        \n    def call(self,query,values):\n        query_with_time_axis = tf.expand_dims(query, 1)\n        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis)+self.W2(values)))\n        attention_weights = tf.nn.softmax(score, axis=1)\n        context_vector = attention_weights*values\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        return context_vector, attention_weights","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:03.856948Z","iopub.execute_input":"2023-05-08T09:36:03.857276Z","iopub.status.idle":"2023-05-08T09:36:03.866203Z","shell.execute_reply.started":"2023-05-08T09:36:03.857243Z","shell.execute_reply":"2023-05-08T09:36:03.864884Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"attention_layer = BahdanauAttention(10)\nattention_result, attention_weights = attention_layer(sam_hidden, sam_output)\n\nprint(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\nprint(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:03.867870Z","iopub.execute_input":"2023-05-08T09:36:03.868256Z","iopub.status.idle":"2023-05-08T09:36:03.944743Z","shell.execute_reply.started":"2023-05-08T09:36:03.868216Z","shell.execute_reply":"2023-05-08T09:36:03.943457Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Attention result shape: (batch size, units) (64, 1024)\nAttention weights shape: (batch_size, sequence_length, 1) (64, 22, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"@keras.utils.register_keras_serializable()\nclass Decoder(tf.keras.Model):\n    def __init__(self, voc_size, emb_dim, dec_units, batch_size):\n        super(Decoder, self).__init__()\n        self.batch_sz = batch_size\n        self.dec_units = dec_units\n        self.embedding = tf.keras.layers.Embedding(voc_size, emb_dim)\n        self.gru = tf.keras.layers.GRU(self.dec_units, \n                                       return_sequences=True, \n                                       return_state=True, \n                                       recurrent_initializer='glorot_uniform')\n        self.fc = tf.keras.layers.Dense(voc_size)\n        self.attention = BahdanauAttention(self.dec_units)\n        \n    def call(self, x, hidden, enc_output):\n        context_vector, attention_weights = self.attention(hidden, enc_output)\n        x = self.embedding(x)\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n        output, state = self.gru(x)\n        output = tf.reshape(output, (-1, output.shape[2]))\n        x = self.fc(output)\n        return x, state, attention_weights","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:03.946343Z","iopub.execute_input":"2023-05-08T09:36:03.947078Z","iopub.status.idle":"2023-05-08T09:36:03.957354Z","shell.execute_reply.started":"2023-05-08T09:36:03.947029Z","shell.execute_reply":"2023-05-08T09:36:03.956348Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"decoder = Decoder(voc_tar_size, emb_dim, units, batch_size)\nsample_decoder_output, _, _ = decoder(tf.random.uniform((batch_size, 1)),sam_hidden, sam_output)\nprint ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:03.958816Z","iopub.execute_input":"2023-05-08T09:36:03.959276Z","iopub.status.idle":"2023-05-08T09:36:04.213986Z","shell.execute_reply.started":"2023-05-08T09:36:03.959240Z","shell.execute_reply":"2023-05-08T09:36:04.212930Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Decoder output shape: (batch_size, vocab size) (64, 2416)\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    return tf.reduce_mean(loss_)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:04.215294Z","iopub.execute_input":"2023-05-08T09:36:04.215857Z","iopub.status.idle":"2023-05-08T09:36:04.225909Z","shell.execute_reply.started":"2023-05-08T09:36:04.215820Z","shell.execute_reply":"2023-05-08T09:36:04.224773Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(inp, targ, enc_hidden):\n    loss=0\n    \n    with tf.GradientTape() as tape:\n        enc_output, enc_hidden = encoder(inp, enc_hidden)\n        dec_hidden = enc_hidden\n        dec_input = tf.expand_dims([target_lang.word_index['<sos>']]*batch_size,1)\n        for t in range(1,targ.shape[1]):\n            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n            loss += loss_function(targ[:,t], predictions)\n            dec_input = tf.expand_dims(targ[:,t],1)\n    \n    batch_loss = (loss/int(targ.shape[1]))\n    variables = encoder.trainable_variables + decoder.trainable_variables\n    gradients = tape.gradient(loss, variables)\n    optimizer.apply_gradients(zip(gradients, variables))\n    return batch_loss","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:04.230059Z","iopub.execute_input":"2023-05-08T09:36:04.230396Z","iopub.status.idle":"2023-05-08T09:36:04.240546Z","shell.execute_reply.started":"2023-05-08T09:36:04.230364Z","shell.execute_reply":"2023-05-08T09:36:04.238974Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"epochs = 50\nfor epoch in range(1, epochs+1):\n    enc_hidden = encoder.initialize_hidden_state()\n    total_loss = 0\n    for (batch,(inp,targ)) in enumerate(ds.take(steps_per_epoch)):\n        batch_loss = train_step(inp, targ, enc_hidden)\n        total_loss += batch_loss\n    print('Epoch:',epoch,'  Loss:',total_loss/steps_per_epoch)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T09:36:04.242404Z","iopub.execute_input":"2023-05-08T09:36:04.243651Z","iopub.status.idle":"2023-05-08T11:53:24.597113Z","shell.execute_reply.started":"2023-05-08T09:36:04.243602Z","shell.execute_reply":"2023-05-08T11:53:24.596058Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Epoch: 1   Loss: tf.Tensor(2.1484346, shape=(), dtype=float32)\nEpoch: 2   Loss: tf.Tensor(1.8243407, shape=(), dtype=float32)\nEpoch: 3   Loss: tf.Tensor(1.6741083, shape=(), dtype=float32)\nEpoch: 4   Loss: tf.Tensor(1.5833634, shape=(), dtype=float32)\nEpoch: 5   Loss: tf.Tensor(1.5118985, shape=(), dtype=float32)\nEpoch: 6   Loss: tf.Tensor(1.4469333, shape=(), dtype=float32)\nEpoch: 7   Loss: tf.Tensor(1.3894451, shape=(), dtype=float32)\nEpoch: 8   Loss: tf.Tensor(1.3420466, shape=(), dtype=float32)\nEpoch: 9   Loss: tf.Tensor(1.293973, shape=(), dtype=float32)\nEpoch: 10   Loss: tf.Tensor(1.2486974, shape=(), dtype=float32)\nEpoch: 11   Loss: tf.Tensor(1.2015507, shape=(), dtype=float32)\nEpoch: 12   Loss: tf.Tensor(1.1563594, shape=(), dtype=float32)\nEpoch: 13   Loss: tf.Tensor(1.111613, shape=(), dtype=float32)\nEpoch: 14   Loss: tf.Tensor(1.0709873, shape=(), dtype=float32)\nEpoch: 15   Loss: tf.Tensor(1.0317986, shape=(), dtype=float32)\nEpoch: 16   Loss: tf.Tensor(0.99663866, shape=(), dtype=float32)\nEpoch: 17   Loss: tf.Tensor(0.9559369, shape=(), dtype=float32)\nEpoch: 18   Loss: tf.Tensor(0.91161245, shape=(), dtype=float32)\nEpoch: 19   Loss: tf.Tensor(0.86874545, shape=(), dtype=float32)\nEpoch: 20   Loss: tf.Tensor(0.82726663, shape=(), dtype=float32)\nEpoch: 21   Loss: tf.Tensor(0.7788204, shape=(), dtype=float32)\nEpoch: 22   Loss: tf.Tensor(0.7323282, shape=(), dtype=float32)\nEpoch: 23   Loss: tf.Tensor(0.68891156, shape=(), dtype=float32)\nEpoch: 24   Loss: tf.Tensor(0.64012825, shape=(), dtype=float32)\nEpoch: 25   Loss: tf.Tensor(0.5931572, shape=(), dtype=float32)\nEpoch: 26   Loss: tf.Tensor(0.544958, shape=(), dtype=float32)\nEpoch: 27   Loss: tf.Tensor(0.49241352, shape=(), dtype=float32)\nEpoch: 28   Loss: tf.Tensor(0.44637355, shape=(), dtype=float32)\nEpoch: 29   Loss: tf.Tensor(0.39910263, shape=(), dtype=float32)\nEpoch: 30   Loss: tf.Tensor(0.3559773, shape=(), dtype=float32)\nEpoch: 31   Loss: tf.Tensor(0.30560628, shape=(), dtype=float32)\nEpoch: 32   Loss: tf.Tensor(0.26348713, shape=(), dtype=float32)\nEpoch: 33   Loss: tf.Tensor(0.22388078, shape=(), dtype=float32)\nEpoch: 34   Loss: tf.Tensor(0.18470886, shape=(), dtype=float32)\nEpoch: 35   Loss: tf.Tensor(0.15141918, shape=(), dtype=float32)\nEpoch: 36   Loss: tf.Tensor(0.12422194, shape=(), dtype=float32)\nEpoch: 37   Loss: tf.Tensor(0.1006208, shape=(), dtype=float32)\nEpoch: 38   Loss: tf.Tensor(0.08128688, shape=(), dtype=float32)\nEpoch: 39   Loss: tf.Tensor(0.068210974, shape=(), dtype=float32)\nEpoch: 40   Loss: tf.Tensor(0.061099652, shape=(), dtype=float32)\nEpoch: 41   Loss: tf.Tensor(0.059327498, shape=(), dtype=float32)\nEpoch: 42   Loss: tf.Tensor(0.04569724, shape=(), dtype=float32)\nEpoch: 43   Loss: tf.Tensor(0.037201844, shape=(), dtype=float32)\nEpoch: 44   Loss: tf.Tensor(0.032472637, shape=(), dtype=float32)\nEpoch: 45   Loss: tf.Tensor(0.027521757, shape=(), dtype=float32)\nEpoch: 46   Loss: tf.Tensor(0.024969397, shape=(), dtype=float32)\nEpoch: 47   Loss: tf.Tensor(0.021816883, shape=(), dtype=float32)\nEpoch: 48   Loss: tf.Tensor(0.020484168, shape=(), dtype=float32)\nEpoch: 49   Loss: tf.Tensor(0.022041911, shape=(), dtype=float32)\nEpoch: 50   Loss: tf.Tensor(0.021984432, shape=(), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('input_.pkl', 'wb') as f:\n    pickle.dump(input_lang, f)\nwith open('target_.pkl', 'wb') as f:\n    pickle.dump(target_lang, f)\n\ntf.saved_model.save(encoder,'encoder')\ntf.saved_model.save(decoder,'decoder')","metadata":{"execution":{"iopub.status.busy":"2023-05-08T11:53:24.598243Z","iopub.execute_input":"2023-05-08T11:53:24.598627Z","iopub.status.idle":"2023-05-08T11:53:33.315676Z","shell.execute_reply.started":"2023-05-08T11:53:24.598593Z","shell.execute_reply":"2023-05-08T11:53:33.314450Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def remove_tags(sent):\n    return sent.split('<start>')[-1].split('<end>')[0]","metadata":{"execution":{"iopub.status.busy":"2023-05-08T11:53:33.321604Z","iopub.execute_input":"2023-05-08T11:53:33.321970Z","iopub.status.idle":"2023-05-08T11:53:33.328735Z","shell.execute_reply.started":"2023-05-08T11:53:33.321937Z","shell.execute_reply":"2023-05-08T11:53:33.327457Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def evaluate(sent):\n    sent = clean_text(sent)\n    inputs = [input_lang.word_index[i] for i in sent.split(' ')]\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_len_inp, padding='post')\n    inputs = tf.convert_to_tensor(inputs)\n    result = ''\n    hidden = [tf.zeros((1,units))]\n    enc_out, enc_hidden = encoder(inputs, hidden)\n    dec_hidden = enc_hidden\n    dec_input = tf.expand_dims([target_lang.word_index['<sos>']],0)\n    for t in range(max_len_target):\n        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n        attention_weights = tf.reshape(attention_weights, (-1, ))\n        predicted_id = tf.argmax(predictions[0]).numpy()\n        result += target_lang.index_word[predicted_id] + ' '\n        if target_lang.index_word[predicted_id] == '<eos>':\n            return remove_tags(result), remove_tags(sent)\n        dec_input = tf.expand_dims([predicted_id],0)\n    return remove_tags(result), remove_tags(sent)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T11:53:33.330469Z","iopub.execute_input":"2023-05-08T11:53:33.331429Z","iopub.status.idle":"2023-05-08T11:53:33.342779Z","shell.execute_reply.started":"2023-05-08T11:53:33.331380Z","shell.execute_reply":"2023-05-08T11:53:33.341615Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def ask(sent):\n    result, sent = evaluate(sent)\n    print('Que: ', sent.replace('<sos> ','').replace(' <eos>',''), '\\nAns: ', result)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T11:53:33.348206Z","iopub.execute_input":"2023-05-08T11:53:33.349406Z","iopub.status.idle":"2023-05-08T11:53:33.355703Z","shell.execute_reply.started":"2023-05-08T11:53:33.349359Z","shell.execute_reply":"2023-05-08T11:53:33.354591Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"q, a = [], []\nwith open('../input/simple-dialogs-for-chatbot/dialogs.txt','r') as f:\n    for line in f:\n        line = line.split('\\t')\n        q.append(line[0])\n        a.append(line[1].replace('\\n',''))","metadata":{"execution":{"iopub.status.busy":"2023-05-08T11:53:33.357274Z","iopub.execute_input":"2023-05-08T11:53:33.357789Z","iopub.status.idle":"2023-05-08T11:53:33.380351Z","shell.execute_reply.started":"2023-05-08T11:53:33.357741Z","shell.execute_reply":"2023-05-08T11:53:33.379031Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"ask('hello')","metadata":{"execution":{"iopub.status.busy":"2023-05-08T11:55:14.310018Z","iopub.execute_input":"2023-05-08T11:55:14.310886Z","iopub.status.idle":"2023-05-08T11:55:14.625968Z","shell.execute_reply.started":"2023-05-08T11:55:14.310836Z","shell.execute_reply":"2023-05-08T11:55:14.624708Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Que:  hello \nAns:  it is a really big and pepper was little work <eos> \n","output_type":"stream"}]}]}